from fastapi import FastAPI
from pydantic import BaseModel
from groq import Groq
import json
import unicodedata
import re
from pathlib import Path
from typing import Dict, Any, Optional, List, Tuple

import numpy as np
from sentence_transformers import SentenceTransformer
from fastapi.middleware.cors import CORSMiddleware

from danh_muc import CATEGORY_MAPPING, THUONG_HIEU_MAPPING
from conn import API_KEY_GROQ

# =====================================
# CONFIG
# =====================================

BASE_DIR = Path(__file__).resolve().parent
DIR_DANH_MUC = BASE_DIR / "danh-muc"
VECTOR_DIR = BASE_DIR / "vector-cache"
VECTOR_DIR.mkdir(exist_ok=True)

PAGE_SIZE = 5
EMBEDDING_MODEL_NAME = "intfloat/multilingual-e5-base"

app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# =====================================
# GROQ CLIENT (LAZY)
# =====================================

_client: Optional[Groq] = None


def get_groq_client() -> Groq:
    global _client
    if _client is None:
        if not API_KEY_GROQ:
            raise RuntimeError("Missing API_KEY_GROQ")
        _client = Groq(api_key=API_KEY_GROQ)
    return _client


# =====================================
# EMBEDDING MODEL (LAZY)
# =====================================

_embedding_model: Optional[SentenceTransformer] = None


def get_embedding_model() -> SentenceTransformer:
    global _embedding_model
    if _embedding_model is None:
        _embedding_model = SentenceTransformer(EMBEDDING_MODEL_NAME)
    return _embedding_model


# =====================================
# GLOBAL STATE
# =====================================

SESSION: Dict[str, Dict[str, Any]] = {}
PRODUCT_CACHE: Dict[str, List[Dict[str, Any]]] = {}
VECTOR_CACHE: Dict[str, np.ndarray] = {}

# =====================================
# INPUT
# =====================================


class ChatRequest(BaseModel):
    session_id: str
    message: str


# =====================================
# UTIL
# =====================================

def normalize(text: str) -> str:
    text = (text or "").lower()
    text = unicodedata.normalize("NFD", text)
    return "".join(c for c in text if unicodedata.category(c) != "Mn")


def is_load_more(msg: str) -> bool:
    msg = normalize(msg)
    return any(k in msg for k in ["xem them", "them nua", "tiep di", "tiep tuc", "cho them", "xem tiep"])


def is_relax_price(msg: str) -> bool:
    msg = normalize(msg)
    return any(k in msg for k in ["khong quan tam gia", "bo gia", "gia nao cung duoc", "khong can gia"])


def is_combo_intent(msg: str) -> bool:
    msg = normalize(msg)
    if is_next_combo(msg):
        return False
    return any(k in msg for k in [
        "combo",
        "bo dung cu",
        "day du",
        "bao gom",
        "set do",
        "tron bo",
        "liet ke",
    ])

def is_next_combo(msg: str) -> bool:
    msg = normalize(msg)
    return any(k in msg for k in [
        "combo khac",
        "cho combo khac",
        "combo nua",
        "cho combo nua",
        "set khac",
        "doi combo",
        "doi set",
    ])


def is_many_combo(msg: str) -> bool:
    msg = normalize(msg)
    return any(k in msg for k in [
        "cho nhieu combo",
        "nhieu combo",
        "liet ke nhieu combo",
        "cho vai combo",
        "combo nhieu hon",
    ])


def has_category_in_message(msg: str) -> bool:
    msg_norm = normalize(msg)
    for raw_key in CATEGORY_MAPPING.keys():
        key_norm = normalize(raw_key)
        pattern = r"\b{}\b".format(re.escape(key_norm))
        if re.search(pattern, msg_norm):
            return True
    return False


# =====================================
# CATEGORY
# =====================================

def detect_category(msg: str) -> Optional[str]:
    """
    Match theo T·ª™ NGUY√äN V·∫∏N b·∫±ng regex \bword\b
    Kh√¥ng match substring (vd: 'cao' != 'ao')
    """
    msg_norm = normalize(msg)

    for raw_key, full_label in CATEGORY_MAPPING.items():
        key_norm = normalize(raw_key)
        pattern = r"\b{}\b".format(re.escape(key_norm))
        if re.search(pattern, msg_norm):
            return normalize(full_label).replace(" ", "-")

    return None


def detect_multi_categories(msg: str) -> List[str]:
    """
    D√≤ nhi·ªÅu category trong 1 c√¢u ƒë·ªÉ l√†m combo.
    """
    msg_norm = normalize(msg)
    slugs: List[str] = []

    for raw_key, full_label in CATEGORY_MAPPING.items():
        key_norm = normalize(raw_key)
        pattern = r"\b{}\b".format(re.escape(key_norm))
        if re.search(pattern, msg_norm):
            slugs.append(normalize(full_label).replace(" ", "-"))

    # unique + gi·ªØ th·ª© t·ª± t∆∞∆°ng ƒë·ªëi
    seen = set()
    out = []
    for x in slugs:
        if x not in seen:
            seen.add(x)
            out.append(x)
    return out


# =====================================
# BRAND + PRICE
# =====================================

def parse_price(val: str, unit: Optional[str]) -> int:
    v = float(val.replace(",", "."))
    if unit in ("tr", "trieu"):
        return int(v * 1_000_000)
    if unit == "k":
        return int(v * 1_000)
    return int(v)


def extract_filters(msg: str) -> Tuple[Optional[str], Optional[int], Optional[int]]:
    raw = msg or ""
    msg_norm = normalize(msg)

    # BRAND: match key ƒë√£ normalize ƒë·ªÉ ch·∫Øc ch·∫Øn
    brand = None
    for k, v in THUONG_HIEU_MAPPING.items():
        k_norm = normalize(k)
        if k_norm and k_norm in msg_norm:
            brand = v
            break

    # CAO H∆†N / TR√äN / L·ªöN H∆†N / >
    m = re.search(r"(cao hon|tren|lon hon|>)\s*(\d+(?:[\.,]\d+)?)\s*(trieu|tr|k)?", msg_norm)
    if m:
        unit = m.group(3) or "vnd"
        return brand, parse_price(m.group(2), unit), None

    # D∆Ø·ªöI / NH·ªé H∆†N / <
    m = re.search(r"(duoi|nho hon|<)\s*(\d+(?:[\.,]\d+)?)\s*(trieu|tr|k)?", msg_norm)
    if m:
        unit = m.group(3) or "vnd"
        return brand, None, parse_price(m.group(2), unit)

    # RANGE: 1tr - 2tr
    m = re.search(
        r"(\d+(?:[\.,]\d+)?)\s*(trieu|tr|k)\s*(den|-)\s*(\d+(?:[\.,]\d+)?)\s*(trieu|tr|k)",
        msg_norm
    )
    if m:
        p1 = parse_price(m.group(1), m.group(2))
        p2 = parse_price(m.group(4), m.group(5))
        return brand, min(p1, p2), max(p1, p2)

    # GI√Å ƒê∆†N (1tr ‚Üí ¬±300k)
    m = re.search(r"(\d+(?:[\.,]\d+)?)\s*(trieu|tr|k)", msg_norm)
    if m:
        center = parse_price(m.group(1), m.group(2))
        return brand, center - 300_000, center + 300_000

    # S·ªë thu·∫ßn (>=6 ch·ªØ s·ªë ‚Üí coi l√† VND)
    digits = re.findall(r"\d{6,9}", raw.replace(" ", ""))
    if digits:
        center = int(digits[0])
        return brand, center - 300_000, center + 300_000

    return brand, None, None


# =====================================
# DATA + VECTOR
# =====================================

def load_products(slug: str) -> List[Dict[str, Any]]:
    if slug in PRODUCT_CACHE:
        return PRODUCT_CACHE[slug]

    path = DIR_DANH_MUC / f"{slug}.json"
    if not path.exists():
        PRODUCT_CACHE[slug] = []
        return []

    with open(path, "r", encoding="utf-8") as f:
        PRODUCT_CACHE[slug] = json.load(f)

    return PRODUCT_CACHE[slug]


def build_product_text(p: Dict[str, Any]) -> str:
    parts = [
        str(p.get("ten_san_pham", "")),
        str(p.get("ten_thuong_hieu", "")),
        str(p.get("ten_danh_muc", "")),
        str(p.get("mo_ta_ngan", "")),
        str(p.get("mo_ta", "")),
    ]
    return " ".join([x for x in parts if x])


def load_vectors(slug: str) -> np.ndarray:
    if slug in VECTOR_CACHE:
        return VECTOR_CACHE[slug]

    vec_path = VECTOR_DIR / f"{slug}.npy"
    products = load_products(slug)
    model = get_embedding_model()

    if vec_path.exists():
        vectors = np.load(vec_path)
        if len(vectors) != len(products):
            texts = [f"passage: {build_product_text(p)}" for p in products]
            vectors = model.encode(texts, normalize_embeddings=True)
            np.save(vec_path, vectors)
    else:
        texts = [f"passage: {build_product_text(p)}" for p in products]
        vectors = model.encode(texts, normalize_embeddings=True)
        np.save(vec_path, vectors)

    VECTOR_CACHE[slug] = vectors
    return vectors


def semantic_search(
    slug: str,
    query: str,
    brand: Optional[str],
    pmin: Optional[int],
    pmax: Optional[int],
    top_k: int = 100,
) -> List[Dict[str, Any]]:
    products = load_products(slug)
    if not products:
        return []

    vectors = load_vectors(slug)
    model = get_embedding_model()

    q_emb = model.encode([f"query: {query}"], normalize_embeddings=True)[0]
    scores = vectors @ q_emb
    idx = np.argsort(-scores)

    results = []
    for i in idx:
        p = products[int(i)]

        if brand and p.get("ten_thuong_hieu") != brand:
            continue

        price = p.get("gia_ban", 0) or 0
        if pmin is not None and price < pmin:
            continue
        if pmax is not None and price > pmax:
            continue

        results.append(p)
        if len(results) >= top_k:
            break

    return results


# =====================================
# GROQ (ANSWER SHORT)
# =====================================

def call_groq(category_label: str, brand: Optional[str]) -> str:
    client = get_groq_client()
    brand_text = brand or "kh√¥ng gi·ªõi h·∫°n"

    return client.chat.completions.create(
        model="llama-3.1-8b-instant",
        messages=[
            {
                "role": "system",
                "content": (
                    "B·∫°n l√† tr·ª£ l√Ω t∆∞ v·∫•n b√°n d·ª•ng c·ª• c·∫ßu l√¥ng c·ªßa Badminton Shop.\n"
                    "CH·ªà tr·∫£ l·ªùi T·ªêI ƒêA 2 d√≤ng, m·ªói d√≤ng 1 c√¢u.\n"
                    "KH√îNG bullet, KH√îNG li·ªát k√™ d√†i, KH√îNG n√™u t√™n s·∫£n ph·∫©m c·ª• th·ªÉ.\n"
                    "D√≤ng 1: ti√™u ch√≠ ch·ªçn ch√≠nh, ƒë√∫ng lo·∫°i s·∫£n ph·∫©m.\n"
                    "D√≤ng 2: k·∫øt th√∫c b·∫±ng g·ª£i √Ω g√µ 'xem th√™m'."
                ),
            },
            {
                "role": "user",
                "content": (
                    f"Kh√°ch ƒëang t√¨m: {category_label}. "
                    f"Th∆∞∆°ng hi·ªáu ∆∞u ti√™n: {brand_text}. "
                    "T∆∞ v·∫•n ng·∫Øn g·ªçn."
                ),
            },
        ],
        max_tokens=80,
        temperature=0.5,
    ).choices[0].message.content.strip()


# =====================================
# SESSION
# =====================================

def default_session():
    return {
        "search_state": {
            "category": None,
            "brand": None,
            "price_min": None,
            "price_max": None,
        },
        "results": [],
        "offset": 0,
    }


def get_category_label(slug: str) -> str:
    for _, v in CATEGORY_MAPPING.items():
        if normalize(v).replace(" ", "-") == slug:
            return v
    return "S·∫£n ph·∫©m c·∫ßu l√¥ng"


# =====================================
# COMBO
# =====================================
COMBO_PER_CATEGORY = 6
COMBO_MAX_RETURN = 12


def default_combo_state():
    return {
        "slugs": [],
        "brand": None,
        "price_min": None,
        "price_max": None,
        "candidates": {},   # slug -> list s·∫£n ph·∫©m
        "combo_index": 0,
        "last_query": "",
    }


def prepare_combo_candidates(
    slugs: List[str],
    query: str,
    brand: Optional[str],
    pmin: Optional[int],
    pmax: Optional[int],
    per_category: int = COMBO_PER_CATEGORY,
) -> Dict[str, List[Dict[str, Any]]]:
    data: Dict[str, List[Dict[str, Any]]] = {}
    for slug in slugs:
        data[slug] = semantic_search(
            slug=slug,
            query=query,
            brand=brand,
            pmin=pmin,
            pmax=pmax,
            top_k=per_category,
        )
    return data


def build_combo_by_index(
    candidates: Dict[str, List[Dict[str, Any]]],
    index: int,
) -> List[Dict[str, Any]]:
    combo: List[Dict[str, Any]] = []
    for slug, items in candidates.items():
        if not items:
            continue
        combo.append(items[index % len(items)])
    return combo


def combo_can_generate(candidates: Dict[str, List[Dict[str, Any]]]) -> bool:
    non_empty = sum(1 for items in candidates.values() if items)
    return non_empty >= 2


def calc_possible_combo_count(candidates: Dict[str, List[Dict[str, Any]]]) -> int:
    lens = [len(items) for items in candidates.values() if items]
    return max(lens) if lens else 0

def build_combo(
    slugs: List[str],
    query: str,
    brand: Optional[str],
    pmin: Optional[int],
    pmax: Optional[int],
) -> List[Dict[str, Any]]:
    """
    Ch·ªçn 1 s·∫£n ph·∫©m/top cho m·ªói danh m·ª•c trong combo.
    (C√≥ th·ªÉ m·ªü r·ªông: nhi·ªÅu l·ª±a ch·ªçn m·ªói danh m·ª•c)
    """
    combo: List[Dict[str, Any]] = []
    for slug in slugs:
        results = semantic_search(
            slug=slug,
            query=query,
            brand=brand,
            pmin=pmin,
            pmax=pmax,
            top_k=1,
        )
        if results:
            combo.append(results[0])
    return combo

def default_session():
    return {
        "search_state": {
            "category": None,
            "brand": None,
            "price_min": None,
            "price_max": None,
        },
        "results": [],
        "offset": 0,
        "combo_state": None,   # üëà th√™m
    }

def is_warranty_policy(msg: str) -> bool:
    msg = normalize(msg)
    keywords = [
        "chinh sach bao hanh",
        "bao hanh",
        "doi tra",
        "doi moi",
        "bao hanh nhu the nao",
        "chinh sach doi tra",
    ]
    return any(k in msg for k in keywords)

WARRANTY_TEXT = (
    "N·∫øu s·∫£n ph·∫©m x·∫£y ra l·ªói nh∆∞ nh·ªØng t√¨nh tr·∫°ng tr√™n, qu√Ω kh√°ch vui l√≤ng th·ª±c hi·ªán c√°c b∆∞·ªõc sau ƒë·ªÉ c·ª≠a h√†ng h·ªó tr·ª£ b·∫£o h√†nh:\n\n"
    "(B∆∞·ªõc 1) Khi ph√°t hi·ªán l·ªói s·∫£n ph·∫©m, qu√Ω kh√°ch vui l√≤ng gi·ªØ nguy√™n hi·ªán tr·∫°ng v√† li√™n h·ªá ngay v·ªõi Shop Badminton "
    "qua LI√äN H·ªÜ V·ªöI CH√öNG T√îI ƒë·ªÉ y√™u c·∫ßu b·∫£o h√†nh.\n\n"
    "(B∆∞·ªõc 2) Qu√Ω kh√°ch vui l√≤ng ƒëi·ªÅn ƒë·∫ßy ƒë·ªß th√¥ng tin bao g·ªìm (th√¥ng tin li√™n h·ªá), "
    "(th√¥ng tin s·∫£n ph·∫©m) v√† (m√¥ t·∫£ chi ti·∫øt l·ªói g·∫∑p ph·∫£i).\n\n"
    "(B∆∞·ªõc 3) Sau khi admin ti·∫øp nh·∫≠n y√™u c·∫ßu b·∫£o h√†nh, ch√∫ng t√¥i s·∫Ω ph·∫£n h·ªìi l·∫°i qua (EMAIL) "
    "m√† qu√Ω kh√°ch ƒë√£ cung c·∫•p. Vui l√≤ng theo d√µi th√¥ng b√°o t·ª´ email.\n\n"
    "(B∆∞·ªõc 4) Trong tr∆∞·ªùng h·ª£p s·∫£n ph·∫©m b·ªã l·ªói do nh√† s·∫£n xu·∫•t, qu√Ω kh√°ch s·∫Ω ƒë∆∞·ª£c (ƒë·ªïi s·∫£n ph·∫©m m·ªõi) "
    "theo ch√≠nh s√°ch b·∫£o h√†nh."
)




# =====================================
# CHAT
# =====================================

@app.post("/chat")
def chat(req: ChatRequest):
    uid = req.session_id
    msg = req.message or ""

    if is_warranty_policy(msg):
        return {"answer": WARRANTY_TEXT, "products": []}

    s = SESSION.get(uid) or default_session()
    st = s["search_state"]

    # relax price
    if is_relax_price(msg) and (st.get("category") or st.get("brand")):
        st["price_min"] = None
        st["price_max"] = None

    # load more (ch·ªâ √°p d·ª•ng flow 1-category c≈©)
    if is_load_more(msg) and s["results"]:
        offset = s["offset"]
        batch = s["results"][offset: offset + PAGE_SIZE]
        s["offset"] += PAGE_SIZE
        SESSION[uid] = s
        return {
            "answer": "M√¨nh g·ª≠i th√™m v√†i m·∫´u n·ªØa.\nG√µ 'xem th√™m' n·∫øu mu·ªën ti·∫øp.",
            "products": batch,
        }

    # =====================================
    # COMBO FLOW (return s·ªõm)
    # =====================================
    if is_next_combo(msg):
        cs = s.get("combo_state")
        if not cs:
            return {
                "answer": "B·∫°n ch∆∞a ch·ªçn combo n√†o tr∆∞·ªõc ƒë√≥.\nH√£y n√≥i 'combo v·ª£t gi√†y balo' nh√©.",
                "products": [],
            }

        cs["combo_index"] += 1
        idx = cs["combo_index"]

        max_combo = calc_possible_combo_count(cs["candidates"])
        if idx >= max_combo:
            return {
                "answer": "M√¨nh ƒë√£ g·ª£i √Ω h·∫øt c√°c combo ph√π h·ª£p.\nB·∫°n mu·ªën ƒë·ªïi y√™u c·∫ßu kh√¥ng?",
                "products": [],
            }

        combo = build_combo_by_index(cs["candidates"], idx)
        SESSION[uid] = s

        return {
            "answer": "ƒê√¢y l√† combo kh√°c ƒë·ªÉ b·∫°n so s√°nh.\nG√µ 'combo kh√°c' n·∫øu mu·ªën xem ti·∫øp.",
            "products": combo,
        }

    if is_combo_intent(msg):
        brand, pmin, pmax = extract_filters(msg)
        slugs = detect_multi_categories(msg)

        if not slugs:
            return {
                "answer": "B·∫°n mu·ªën combo g·ªìm v·ª£t, gi√†y, balo hay qu·∫ßn √°o?\nB·∫°n h√£y li·ªát k√™ gi√∫p m√¨nh.",
                "products": [],
            }

        candidates = prepare_combo_candidates(
            slugs=slugs,
            query=msg,
            brand=brand,
            pmin=pmin,
            pmax=pmax,
        )

        if not combo_can_generate(candidates):
            return {
                "answer": "Kh√¥ng ƒë·ªß s·∫£n ph·∫©m ƒë·ªÉ t·∫°o combo ph√π h·ª£p.\nB·∫°n th·ª≠ ƒë·ªïi y√™u c·∫ßu nh√©.",
                "products": [],
            }

        combo = build_combo_by_index(candidates, 0)

        s["combo_state"] = {
            "slugs": slugs,
            "brand": brand,
            "price_min": pmin,
            "price_max": pmax,
            "candidates": candidates,
            "combo_index": 0,
            "last_query": msg,
        }

        SESSION[uid] = s

        return {
            "answer": "M√¨nh g·ª£i √Ω 1 combo ph√π h·ª£p ƒë·ªÉ b·∫Øt ƒë·∫ßu.\nG√µ 'combo kh√°c' ƒë·ªÉ xem b·ªô kh√°c.",
            "products": combo,
        }


    # =====================================
    # FLOW C≈®: 1 CATEGORY
    # =====================================

    # detect category
    cat = detect_category(msg)
    if cat:
        if st["category"] != cat:
            st["brand"] = None
            st["price_min"] = None
            st["price_max"] = None
        st["category"] = cat

    # brand + price
    brand, pmin, pmax = extract_filters(msg)
    if brand is not None:
        st["brand"] = brand
    if pmin is not None:
        st["price_min"] = pmin
    if pmax is not None:
        st["price_max"] = pmax

    # no category yet
    if not st["category"]:
        return {
            "answer": "B·∫°n mu·ªën t√¨m v·ª£t, gi√†y, √°o, qu·∫ßn hay balo?\nH√£y n√≥i r√µ danh m·ª•c gi√∫p m√¨nh.",
            "products": [],
        }

    slug = st["category"]

    # search
    filtered = semantic_search(
        slug=slug,
        query=msg,
        brand=st["brand"],
        pmin=st["price_min"],
        pmax=st["price_max"],
        top_k=100,
    )

    if not filtered:
        return {
            "answer": "Kh√¥ng c√≥ s·∫£n ph·∫©m ph√π h·ª£p theo b·ªô l·ªçc hi·ªán t·∫°i.\nB·∫°n th·ª≠ n·ªõi l·ªèng gi√° ho·∫∑c ƒë·ªïi th∆∞∆°ng hi·ªáu.",
            "products": [],
        }

    # save
    s["results"] = filtered
    s["offset"] = PAGE_SIZE
    SESSION[uid] = s

    # groq
    answer = call_groq(get_category_label(slug), st["brand"])

    return {
        "answer": answer,
        "products": filtered[:PAGE_SIZE],
    }
